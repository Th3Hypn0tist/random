# JIT Symbolic Memory
## A Design Pattern for AI Systems

Author: Aki Hirvilammi  
Version: 1.0  
Status: Design Pattern (Conceptual)

---

## Licensing Notice

Reading this document is free.

Use of the **JIT Symbolic Memory** architecture in any system requires
an explicit license.

- **Private and internal use** is permitted under the **One Pizza License (OPL)**.
- **Service-based use**, including SaaS, hosted platforms, enterprise systems,
  or client-delivered solutions, is **explicitly excluded from OPL**
  and requires a separate commercial license.

Licensing terms are defined in Section 11.

---

## Important Notice

This document describes a **design pattern**, not an implementation.

It is intentionally:
- incomplete
- non-operational
- non-prescriptive

It MUST NOT be treated as:
- an implementation guide
- a technical specification
- a reference architecture

Any system claiming to “implement JIT Symbolic Memory” based solely on this
document is, by definition, incomplete.

Real systems require additional architectural decisions, constraints,
and safeguards not described here.

---

## 1. The Memory Illusion in LLM Systems

Modern LLM-based systems often appear to “remember”.

They do not.

This illusion is caused by a structural error:
**conflating context with memory**.

Context is:
- ephemeral
- token-bound
- continuously overwritten
- non-addressable
- non-auditable

Treating context as memory produces systems that:
- drift over time
- behave inconsistently
- fail under audit
- degrade under scale

**LLMs do not have memory.**
They operate within temporary context windows.

---

## 2. The Foundational Separation

JIT Symbolic Memory begins with a strict, non-negotiable distinction:

> **Context ≠ Memory**

They serve different purposes and obey different constraints.

- Context enables *immediate reasoning*
- Memory preserves *structured knowledge over time*

Any architecture that blurs this boundary may appear intelligent,
but will eventually fail.

---

## 3. The Three-Layer Model

JIT Symbolic Memory defines a minimal, explicit, three-layer structure.

### 3.1 Context Layer

- Ephemeral
- Token-limited
- Non-persistent
- Continuously replaced
- Used only for active reasoning

Context is **never** a storage medium.

---

### 3.2 Symbolic Memory Layer

- Persistent
- Explicitly written
- Explicitly retrieved
- Addressable by identifier
- Independent of model context

This layer stores **symbols**, not interpretations.

---

### 3.3 Abstraction Layer (ABS)

- Organizes symbolic memory into conceptual structures
- Provides semantic framing without embedding interpretation
- Decouples memory from individual conversations or agents
- Enables reuse across tasks, agents, and time

ABS defines structure — not meaning-in-use.

---

## 4. The Just-In-Time Principle

Memory is never injected by default.

**Symbolic memory is retrieved only when explicitly required.**

There is:
- no background recall
- no implicit accumulation
- no hidden prompt growth

Memory enters context:
- intentionally
- temporarily
- for a specific reasoning step

This preserves:
- determinism
- controllability
- auditability
- cost discipline

---

## 5. The Role of the LLM

Within this pattern, the LLM is **not** a memory system.

The LLM functions as:
- a reasoning engine
- an interpretation engine
- a natural-language interface

The LLM does **not**:
- own memory
- persist state
- decide what must be remembered
- mutate long-term knowledge

All memory operations are explicit and external.

---

## 6. Anti-Patterns

The following approaches directly conflict with JIT Symbolic Memory.

### 6.1 Context Stuffing
Using larger context windows to simulate memory.

### 6.2 Vector Databases as Memory
Replacing symbolic recall with similarity search.

### 6.3 Implicit Recall
Assuming the model will “remember” without explicit retrieval.

### 6.4 Hidden Prompt State
Embedding system state inside evolving prompts.

### 6.5 Emergent Memory Assumptions
Expecting memory to arise automatically from scale or training.

These approaches may appear functional.
They do not scale reliably.

---

## 7. What This Pattern Is Not

JIT Symbolic Memory is not:
- AGI
- a learning mechanism
- a database replacement
- prompt engineering
- a framework or library

It does not address:
- alignment
- ethics
- planning
- autonomy

It addresses exactly one problem:

> **The structural misuse of context as memory.**

---

## 8. Architectural Implications

Adopting this pattern enables:
- model-agnostic system design
- long-lived agent architectures
- deterministic reasoning environments
- auditable behavior
- clear separation of cognition and storage

The architectural shift is simple:

> Not “the model remembers”  
> but “the system remembers”.

---

## 9. Citation

If you build systems inspired by or aligned with this pattern, cite as:

> Hirvilammi, A.  
> *JIT Symbolic Memory: A Design Pattern for AI Systems.*

---

## 10. FAQ

### Do models get worse the more you use them?

No.

Contexts do.

Implicit, unscoped, agent-local state accumulates over time:
- residual assumptions
- prompt drift
- hidden constraints

This creates the illusion of model degradation.

When context is reset, quality returns.

Models do not decay.
Unstructured memory does.

---

## 11. Licensing

### 11.1 License Classes

This architecture is **not free to use by default**.

Two license classes exist:

1. **One Pizza License (OPL)** — private and internal use only  
2. **Commercial / Enterprise License** — service-based or external use

---

### 11.2 One Pizza License (OPL) — Scope

OPL permits **private and internal use only**, including:

- personal projects
- internal research and development
- internal tools
- non-hosted, single-organization deployments

OPL explicitly **does NOT permit**:

- SaaS or hosted services
- APIs offered to external users
- enterprise platforms
- client-delivered systems
- multi-tenant environments
- revenue-generating services
- sublicensing or resale

Any use where the architecture is part of a **service offered to others**
requires a **separate commercial license**.

---

### 11.3 One Pizza License (OPL) — Fee Definition

- One-time payment
- One (1) pizza per end user
- The pizza price is defined as the **average retail price of a standard pizza**
  in the country where the end user is physically located at the time of payment
- No recurring fees

**Reference definition**

The “average pizza price” means a reasonable market average
(e.g. a common restaurant or delivery pizza, not premium or specialty pricing)
within that country.

A good-faith estimate is sufficient.
Exact pricing precision is not required.

---

### 11.4 Payment & Proof

- Network: Cronos (EVM)
- Token: any Cronos-supported token
- Address:

  0xAddc61aF05ACc594623c3e73D242C17d169A28b2

A confirmed on-chain transaction is sufficient proof of payment.
No registration or reporting is required.

---

### 11.5 Commercial / Enterprise Licensing

Any use where the architecture is:
- embedded in a product
- offered as a service
- deployed for clients
- used in enterprise or multi-tenant systems

requires a **separate commercial or enterprise license**.

OPL does not apply.

---

### 11.6 Attribution

All public use must include attribution to the original author
and a reference to the original document or repository.

---

## Closing Statement

JIT Symbolic Memory does not make models smarter.

It makes systems **structurally honest**.
